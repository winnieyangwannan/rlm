{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95b5e72f",
   "metadata": {},
   "source": [
    "# RLM Log Analysis Functions\n",
    "\n",
    "This notebook provides utility functions to extract key data from RLM log files:\n",
    "- **Final answer**: The agent's concluding response\n",
    "- **Code blocks**: All code executed during the session\n",
    "- **RLM calls**: Sub-LLM calls made via `llm_query()` / `llm_query_batched()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66628fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "\n",
    "\n",
    "sys.path.append('data/analysis')\n",
    "import rlm_log_utils\n",
    "importlib.reload(rlm_log_utils)\n",
    "from rlm_log_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94b4e70",
   "metadata": {},
   "source": [
    "## Usage Example\n",
    "\n",
    "Load the log file and extract key information:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1754fc72",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f328b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 iterations\n"
     ]
    }
   ],
   "source": [
    "# LOG_PATH = \"/checkpoint/maui_sft/winnieyangwn/rlm_dumps/gpt-5_common_invalid_errors_513_2026-02-02_01-19-55_0c5414d6.jsonl\"\n",
    "# LOG_PATH = \"/checkpoint/maui_sft/winnieyangwn/rlm_dumps/gpt-5_common_invalid_errors_513_2026-02-02_00-40-36_52897c45.jsonl\"\n",
    "# LOG_PATH = \"/checkpoint/maui_sft/winnieyangwn/rlm_dumps/gpt-5_common_invalid_errors_513_2026-02-02_02-49-49_405164e2.jsonl\"\n",
    "# LOG_PATH = \"/checkpoint/maui_sft/winnieyangwn/rlm_dumps/gpt-5_common_invalid_errors_513_2026-02-02_03-32-48_4e207f16.jsonl\"\n",
    "# LOG_PATH = \"/checkpoint/maui_sft/winnieyangwn/rlm_dumps/gpt-5_common_invalid_errors_513_2026-02-02_05-04-09_a2790315.jsonl\"\n",
    "# LOG_PATH = \"/checkpoint/maui_sft/winnieyangwn/rlm_dumps/gpt-5_common_invalid_errors_513_2026-02-02_05-28-19_99d053cc.jsonl\"\n",
    "# LOG_PATH = \"/checkpoint/maui_sft/winnieyangwn/rlm_dumps/gpt-5_common_invalid_errors_513_2026-02-02_05-48-35_1fcdaf63.jsonl\"\n",
    "LOG_PATH = \"/checkpoint/maui_sft/winnieyangwn/rlm_dumps/gpt-5_common_invalid_errors_codebase_513_2026-02-03_09-43-33_7f873403.jsonl\"\n",
    "# Load the log - first entry is metadata, rest are iterations\n",
    "entries = load_rlm_log(LOG_PATH)\n",
    "metadata = entries[0]\n",
    "iterations = entries[1:]\n",
    "\n",
    "print(f\"Loaded {len(iterations)} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a88249",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2f17c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== METADATA ===\n",
      "type: metadata\n",
      "timestamp: 2026-02-03T09:43:33.745971\n",
      "root_model: gpt-5\n",
      "max_depth: 2\n",
      "max_iterations: 100\n",
      "backend: azure_openai\n",
      "environment_type: local\n",
      "environment_kwargs: {'setup_code': '\\nimport pandas as pd\\nfrom pathlib import Path\\n\\n# Load rollout data as DataFrame\\nrollout_df = pd.read_json(\\'/checkpoint/maui_sft/winnieyangwn/amaia_dumps/513/trajectories/513_metadata.jsonl\\', lines=True)\\n\\n# Load codebase into dict\\ncodebase = {}\\ncodebase_root = Path(\\'/checkpoint/agentic-models/winnieyangwn/amaia_dumps/503/code/2026_02_02_00_55_44\\')\\nfor ext in [\\'.py\\', \\'.md\\', \\'.yaml\\']:\\n    for path in codebase_root.rglob(f\\'*{ext}\\'):\\n        try:\\n            rel_path = str(path.relative_to(codebase_root))\\n            codebase[rel_path] = path.read_text(errors=\\'ignore\\')\\n        except Exception:\\n            pass  # Skip unreadable files\\n\\n# Load config YAML\\nconfig_yaml = \\'\\'\\'# gpt5 after rate limit fix + using jupyter eval as baseline\\n# python -m launchers.stool run name=\"513\" script=apps.sea.eval config=apps/sea/configs/winnieyang/eval/baseline/gpt5/513.yaml nodes=3 group=maui_sft qos=h200_maui_sft_high dirs_exists_ok=True\\n\\n\\n\\n# Output directory for evaluation results\\ndump_dir: /checkpoint/maui_sft/winnieyangwn/amaia_dumps/513\\n# Generation backend configuration\\ngen_backend: litellm\\n\\nlitellm_args:\\n  model: azure/gpt-5\\n  api_key: \"6524db61b4774663a00ba80558122ceb\"\\n  base_url: https://azure-services-fair-openai1-eastus2n3.azure-api.net/openai/deployments/gpt-5\\n  api_version: 2025-03-01-preview\\n  tools_env: mle_bench_bash_env\\n  max_retries: 64\\n  retry_min_wait: 4.0\\n  # retry_max_wait: 60.0\\n\\ngen_args:\\n  use_sampling: true\\n  temperature: 1.0\\n  top_p: 0.95\\n  max_batch: 8\\n  max_gen: 32768\\n\\n\\n# Tokenizer configuration (matching the training config)\\ntokenizer:\\n  name: cwm_instruct\\n  path: /engshare/jcaudet/amaia_checkpoints/cwm/tokenizer.model\\n\\n# Evaluation tasks configuration - using the new mle_bench_bash environment\\ntasks:\\n  - env_config: mle_bench_bash\\n    reward_fn: mle_bench\\n    path: /checkpoint/maui_sft/shared/kniu/datasets/mlebench_full.jsonl  \\n    samples_per_prompt: 64\\n    init_args:\\n      config:\\n        model: \"gpt5\"\\n        prompt_file: \"gpt5-513\"\\n        think: false\\n        context_size: 98304\\n        max_turns: 128\\n        max_action_len: 16384\\n        backend: agentbox\\n        agentbox_manager_uri:  h200-137-003-080:46725 # CHANGE: Need to update every run\\n        session_timeout: 1200.0\\n        eval_timeout: 32400\\n        eval_execution_mode: jupyter  # Options: bash, jupyter - controls how solution.py is executed during evaluation\\n        use_think_tag: false\\n        training: false\\n        benchmark: mlebench\\n    metrics_spec:\\n      pass:\\n        - \"@1\"\\n      execution_outcome:\\n        - \"@1\"\\n\\n# Runtime configuration\\nnum_rollout_threads: 8  # CHANGE: based on num workers an AgentBox server size\\ndata_queue_size: 10000\\nseed: 42\\nperf_log_freq: 60.0\\n\\n# Dump configuration\\ndump_mode: minimal  # Options: full, minimal, none\\ndump_compress: false\\nkeep_start_args: [\"q0_monte_carlo\", \"s0\", \"task_id\", \"instance_id\"]\\nrun_metrics_aggregation: true\\n\\n# Logging configuration\\nlogging:\\n  enable_tensorboard: false\\n  enable_wandb: false\\n  wandb:\\n    entity: none\\n    project: none\\n    name: none\\n\\n# System configuration\\nsetup:\\n  spawn_method: forkserver\\n  torch_init_timeout: 600\\n  cuda_matmul_allow_tf32: true\\n\\nlog_level: debug\\nmax_exceptions: 3\\'\\'\\'\\n\\nprint(f\"Loaded {len(rollout_df)} rollouts, {len(codebase)} codebase files, and config YAML\")\\n'}\n",
      "other_backends: None\n"
     ]
    }
   ],
   "source": [
    "# View metadata\n",
    "print(\"=== METADATA ===\")\n",
    "for k, v in metadata.items():\n",
    "    if k != \"backend_kwargs\":\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba91dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp-based runtime: 182.70s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compare with timestamp-based runtime\n",
    "runtime = get_total_runtime(entries)\n",
    "print(f\"Timestamp-based runtime: {runtime.total_seconds():.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5627fc3",
   "metadata": {},
   "source": [
    "# Final Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2e39a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL ANSWER ===\n",
      "Error: Variable 'final_answer' not found\n"
     ]
    }
   ],
   "source": [
    "# Get the final answer\n",
    "final_answer = get_final_answer(iterations)\n",
    "print(\"=== FINAL ANSWER ===\")\n",
    "print(final_answer if final_answer else \"No final answer found\")\n",
    "# print(f\"\\n(Total length: {len(final_answer) if final_answer else 0} chars)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02938637",
   "metadata": {},
   "source": [
    "# Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bb5e64b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Explore the DataFrame structure and compute the answer\\nimport pandas as pd\\n\\n# Basic checks\\nn_rows = len(context)\\ncols = list(context.columns)\\nvalid_counts = context[\"valid_submission\"].value_counts(dropna=False)\\n\\n# Filter invalid submissions\\ninvalid = context[context[\"valid_submission\"] == False]\\n\\n# Get top 5 most common evaluation error messages (strip whitespace, drop missing)\\ntop_errors = (\\n    invalid[\"eval_error_output\"]\\n    .dropna()\\n    .map(lambda s: s.strip())\\n    .value_counts()\\n    .head(5)\\n)\\n\\nprint(\"Total rows:\", n_rows)\\nprint(\"Columns:\", cols)\\nprint(\"Valid vs Invalid counts:\\\\n\", valid_counts)\\nprint(\"\\\\nTop 5 most common eval_error_output among invalid submissions:\")\\nprint(top_errors.to_string())'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations[0][\"code_blocks\"][0][\"code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "18b01308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['f', 'context_0', 'context', 'pd', 'n_rows', 'cols', 'valid_counts', 'invalid', 'top_errors'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations[0][\"code_blocks\"][0][\"result\"][\"locals\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e990a6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(iterations[0][\"code_blocks\"][0][\"result\"][\"locals\"][\"context_0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "91aa3ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.657848205417395"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations[0][\"iteration_time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199c4234",
   "metadata": {},
   "source": [
    "# Iteration #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "10045c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.49644836317748"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations[1][\"iteration_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7c8aa9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```repl\\nimport pandas as pd\\nimport re\\n\\n# Recompute value counts for safety\\ninvalid_mask = context[\"valid_submission\"] == False\\ninvalid_df = context.loc[invalid_mask, [\"eval_error_output\"]].copy()\\ninvalid_df[\"eval_error_output_norm\"] = (\\n    invalid_df[\"eval_error_output\"]\\n    .astype(str)\\n    .str.strip()\\n)\\n\\nclean_invalid = invalid_df[\\n    (invalid_df[\"eval_error_output_norm\"].notna()) &\\n    (invalid_df[\"eval_error_output_norm\"] != \"\") &\\n    (invalid_df[\"eval_error_output_norm\"] != \"None\") &\\n    (invalid_df[\"eval_error_output_norm\"] != \"nan\")\\n]\\n\\nvc = clean_invalid[\"eval_error_output_norm\"].value_counts()\\ntop5 = vc.head(5)\\n\\ndef summarize_message(msg: str) -> str:\\n    # If it\\'s the common IPython SystemExit message, just keep \"SystemExit: X\"\\n    if msg.startswith(\"SystemExit:\"):\\n        return msg.splitlines()[0].strip()\\n    # If it\\'s the Kaggle validation wrapper, extract the actual error line after the preamble\\n    if msg.startswith(\"Validation error: Submission invalid!\"):\\n        # Extract the line after \"...error message:\\\\n\"\\n        parts = msg.split(\"error message:\\\\n\", 1)\\n        if len(parts) == 2:\\n            rest = parts[1]\\n            actual = rest.splitlines()[0].strip()\\n            return f\"Validation error: {actual}\"\\n        return \"Validation error\"\\n    # Otherwise, keep the first line\\n    return msg.splitlines()[0].strip()\\n\\nsummary_rows = [(summarize_message(msg), int(count)) for msg, count in top5.items()]\\nsummary_rows\\n```\\n'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations[1][\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8aa9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Among invalid submissions (n=1,890), the top 5 most common evaluation error messages are:\\n- SystemExit: 2 — 1,669\\n- Error tokenizing data. C error: Expected 141 fields in line 3, saw 334 — 49\\n- <empty> — 23\\n- ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2. — 16\\n- RuntimeError: Encountered NaN. — 11'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8ad8488",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57ac6f67",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecc94779",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d06f8599",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffb0f077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CODE BLOCKS (1 total) ===\n",
      "\n",
      "--- Block 1 (Iteration 1) ---\n",
      "total = len(context)\n",
      "valid = sum(1 for r in context if r.get(\"valid_submission\"))\n",
      "percentage = (valid / total) * 100\n",
      "print(f\"Total rollouts: {total}, Valid submissions: {valid}, Percentage: {percentage:.2f}%\")\n",
      "\n",
      "[stdout]: Total rollouts: 4800, Valid submissions: 3806, Percentage: 79.29%\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all code blocks\n",
    "code_blocks = get_all_code_with_results(iterations)\n",
    "print(f\"=== CODE BLOCKS ({len(code_blocks)} total) ===\\n\")\n",
    "\n",
    "for i, block in enumerate(code_blocks[:3]):  # Show first 3\n",
    "    print(f\"--- Block {i+1} (Iteration {block['iteration']}) ---\")\n",
    "    print(block[\"code\"][:500])\n",
    "    if block.get(\"stdout\"):\n",
    "        print(f\"\\n[stdout]: {block['stdout'][:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b93a78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RLM CALLS SUMMARY ===\n",
      "Total sub-LLM calls: 0\n",
      "Total input tokens: 0\n",
      "Total output tokens: 0\n",
      "Models used: []\n",
      "Calls per iteration: {}\n"
     ]
    }
   ],
   "source": [
    "# Get RLM calls summary\n",
    "summary = get_sub_rlm_calls_summary(iterations)\n",
    "print(\"=== RLM CALLS SUMMARY ===\")\n",
    "print(f\"Total sub-LLM calls: {summary['total_calls']}\")\n",
    "print(f\"Total input tokens: {summary['total_input_tokens']:,}\")\n",
    "print(f\"Total output tokens: {summary['total_output_tokens']:,}\")\n",
    "print(f\"Models used: {summary['models_used']}\")\n",
    "print(f\"Calls per iteration: {summary['calls_by_iteration']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2bb728b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RLM CALLS DETAIL (0 calls) ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get detailed RLM calls\n",
    "rlm_calls = get_sub_rlm_calls(iterations)\n",
    "print(f\"=== RLM CALLS DETAIL ({len(rlm_calls)} calls) ===\\n\")\n",
    "\n",
    "for i, call in enumerate(rlm_calls[:2]):  # Show first 2 calls\n",
    "    print(f\"--- Call {i+1} (Iteration {call['iteration']}, Block {call['code_block_idx']}) ---\")\n",
    "    print(f\"Model: {call['root_model']}\")\n",
    "    print(f\"Execution time: {call['execution_time']:.2f}s\")\n",
    "    prompt_preview = str(call['prompt'])[:300]\n",
    "    print(f\"Prompt preview: {prompt_preview}...\")\n",
    "    response_preview = call['response'][:300] if call['response'] else \"None\"\n",
    "    print(f\"Response preview: {response_preview}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4786b3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FULL EXTRACTION ===\n",
      "Metadata keys: ['type', 'timestamp', 'root_model', 'max_depth', 'max_iterations', 'backend', 'backend_kwargs', 'environment_type', 'environment_kwargs', 'other_backends']\n",
      "Number of iterations: 2\n",
      "Number of code blocks: 1\n",
      "Number of RLM calls: 0\n",
      "Has final answer: True\n"
     ]
    }
   ],
   "source": [
    "# Extract everything at once\n",
    "all_data = extract_all(LOG_PATH)\n",
    "print(\"=== FULL EXTRACTION ===\")\n",
    "print(f\"Metadata keys: {list(all_data['metadata'].keys()) if all_data['metadata'] else 'None'}\")\n",
    "print(f\"Number of iterations: {all_data['num_iterations']}\")\n",
    "print(f\"Number of code blocks: {len(all_data['code_blocks'])}\")\n",
    "print(f\"Number of RLM calls: {len(all_data['rlm_calls'])}\")\n",
    "print(f\"Has final answer: {all_data['final_answer'] is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4f7601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
