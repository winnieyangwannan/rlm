{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95b5e72f",
   "metadata": {},
   "source": [
    "#  Prompt Revision\n",
    "\n",
    "- This is offline setting for re-writing the whole prompt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66628fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "\n",
    "\n",
    "sys.path.append('/home/winnieyangwn/rlm/analysis')\n",
    "import rlm_log_utils\n",
    "importlib.reload(rlm_log_utils)\n",
    "from rlm_log_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94b4e70",
   "metadata": {},
   "source": [
    "## Usage Example\n",
    "\n",
    "Load the log file and extract key information:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1754fc72",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f328b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 iterations\n"
     ]
    }
   ],
   "source": [
    "# LOG_PATH = \"/checkpoint/maui_sft/winnieyangwn/rlm_dumps/gpt-5_common_invalid_errors_513_2026-02-02_01-19-55_0c5414d6.jsonl\"\n",
    "# LOG_PATH = \"/checkpoint/maui_sft/winnieyangwn/rlm_dumps/gpt-5_common_invalid_errors_513_2026-02-02_00-40-36_52897c45.jsonl\"\n",
    "# LOG_PATH = \"/checkpoint/maui_sft/winnieyangwn/rlm_dumps/gpt-5_common_invalid_errors_513_2026-02-02_02-49-49_405164e2.jsonl\"\n",
    "# LOG_PATH = \"/checkpoint/maui_sft/winnieyangwn/rlm_dumps/gpt-5_common_invalid_errors_513_2026-02-02_03-32-48_4e207f16.jsonl\"\n",
    "# LOG_PATH = \"/checkpoint/maui_sft/winnieyangwn/rlm_dumps/gpt-5_common_invalid_errors_513_2026-02-02_05-04-09_a2790315.jsonl\"\n",
    "# LOG_PATH = \"/checkpoint/maui_sft/winnieyangwn/rlm_dumps/gpt-5_common_invalid_errors_513_2026-02-02_05-28-19_99d053cc.jsonl\"\n",
    "# LOG_PATH = \"/checkpoint/maui_sft/winnieyangwn/rlm_dumps/gpt-5_common_invalid_errors_513_2026-02-02_05-48-35_1fcdaf63.jsonl\"\n",
    "LOG_PATH = \"/checkpoint/maui_sft/winnieyangwn/rlm_dumps/gpt-5_common_invalid_errors_codebase_prompt_513_2026-02-02_07-52-23_71cb44a1.jsonl\" #WORKING\n",
    "# LOG_PATH = \"/checkpoint/maui_sft/winnieyangwn/rlm_dumps/gpt-5_common_invalid_errors_codebase_prompt_513_2026-02-03_09-23-31_e57cfa35.jsonl\"\n",
    "LOG_PATH = \"/checkpoint/maui_sft/winnieyangwn/rlm_dumps/gpt-5_common_invalid_errors_codebase_prompt_513_2026-02-03_19-53-29_6c27ba9b.jsonl\"\n",
    "LOG_PATH = \"/checkpoint/maui_sft/winnieyangwn/rlm_dumps/gpt-5_common_invalid_errors_codebase_prompt_513_2026-02-03_20-34-44_c35a8934.jsonl\"\n",
    "# Load the log - first entry is metadata, rest are iterations\n",
    "entries = load_rlm_log(LOG_PATH)\n",
    "metadata = entries[0]\n",
    "iterations = entries[1:]\n",
    "\n",
    "print(f\"Loaded {len(iterations)} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a88249",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2f17c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== METADATA ===\n",
      "type: metadata\n",
      "timestamp: 2026-02-03T20:34:44.723008\n",
      "root_model: gpt-5\n",
      "max_depth: 2\n",
      "max_iterations: 100\n",
      "backend: azure_openai\n",
      "environment_type: local\n",
      "environment_kwargs: {'setup_code': '\\nimport pandas as pd\\nfrom pathlib import Path\\n\\n# Load rollout data as DataFrame\\nrollout_df = pd.read_json(\\'/checkpoint/maui_sft/winnieyangwn/amaia_dumps/513/trajectories/513_metadata.jsonl\\', lines=True)\\n\\n# Load codebase into dict\\ncodebase = {}\\ncodebase_root = Path(\\'/checkpoint/agentic-models/winnieyangwn/amaia_dumps/503/code/2026_02_02_00_55_44\\')\\nfor ext in [\\'.py\\', \\'.md\\', \\'.yaml\\']:\\n    for path in codebase_root.rglob(f\\'*{ext}\\'):\\n        try:\\n            rel_path = str(path.relative_to(codebase_root))\\n            codebase[rel_path] = path.read_text(errors=\\'ignore\\')\\n        except Exception:\\n            pass  # Skip unreadable files\\n\\n# Load config YAML\\nconfig_yaml = \\'\\'\\'# gpt5 after rate limit fix + using jupyter eval as baseline\\n# python -m launchers.stool run name=\"513\" script=apps.sea.eval config=apps/sea/configs/winnieyang/eval/baseline/gpt5/513.yaml nodes=3 group=maui_sft qos=h200_maui_sft_high dirs_exists_ok=True\\n\\n\\n\\n# Output directory for evaluation results\\ndump_dir: /checkpoint/maui_sft/winnieyangwn/amaia_dumps/513\\n# Generation backend configuration\\ngen_backend: litellm\\n\\nlitellm_args:\\n  model: azure/gpt-5\\n  api_key: \"6524db61b4774663a00ba80558122ceb\"\\n  base_url: https://azure-services-fair-openai1-eastus2n3.azure-api.net/openai/deployments/gpt-5\\n  api_version: 2025-03-01-preview\\n  tools_env: mle_bench_bash_env\\n  max_retries: 64\\n  retry_min_wait: 4.0\\n  # retry_max_wait: 60.0\\n\\ngen_args:\\n  use_sampling: true\\n  temperature: 1.0\\n  top_p: 0.95\\n  max_batch: 8\\n  max_gen: 32768\\n\\n\\n# Tokenizer configuration (matching the training config)\\ntokenizer:\\n  name: cwm_instruct\\n  path: /engshare/jcaudet/amaia_checkpoints/cwm/tokenizer.model\\n\\n# Evaluation tasks configuration - using the new mle_bench_bash environment\\ntasks:\\n  - env_config: mle_bench_bash\\n    reward_fn: mle_bench\\n    path: /checkpoint/maui_sft/shared/kniu/datasets/mlebench_full.jsonl  \\n    samples_per_prompt: 64\\n    init_args:\\n      config:\\n        model: \"gpt5\"\\n        prompt_file: \"gpt5-513\"\\n        think: false\\n        context_size: 98304\\n        max_turns: 128\\n        max_action_len: 16384\\n        backend: agentbox\\n        agentbox_manager_uri:  h200-137-003-080:46725 # CHANGE: Need to update every run\\n        session_timeout: 1200.0\\n        eval_timeout: 32400\\n        eval_execution_mode: jupyter  # Options: bash, jupyter - controls how solution.py is executed during evaluation\\n        use_think_tag: false\\n        training: false\\n        benchmark: mlebench\\n    metrics_spec:\\n      pass:\\n        - \"@1\"\\n      execution_outcome:\\n        - \"@1\"\\n\\n# Runtime configuration\\nnum_rollout_threads: 8  # CHANGE: based on num workers an AgentBox server size\\ndata_queue_size: 10000\\nseed: 42\\nperf_log_freq: 60.0\\n\\n# Dump configuration\\ndump_mode: minimal  # Options: full, minimal, none\\ndump_compress: false\\nkeep_start_args: [\"q0_monte_carlo\", \"s0\", \"task_id\", \"instance_id\"]\\nrun_metrics_aggregation: true\\n\\n# Logging configuration\\nlogging:\\n  enable_tensorboard: false\\n  enable_wandb: false\\n  wandb:\\n    entity: none\\n    project: none\\n    name: none\\n\\n# System configuration\\nsetup:\\n  spawn_method: forkserver\\n  torch_init_timeout: 600\\n  cuda_matmul_allow_tf32: true\\n\\nlog_level: debug\\nmax_exceptions: 3\\'\\'\\'\\n\\n# Load prompt\\nprompt = \\'\\'\\'SYSTEM_PROMPT = \"\"\"\\nYou are an AUTONOMOUS ML engineering agent. Your goal is to develop an innovative solution that trains a model and generates predictions for a Kaggle-style competition.\\n\\n<precedence_rules>\\nIf instructions conflict, follow this order:\\n1. This prompt\\'s requirements\\n2. Original competition instructions\\n</precedence_rules>\\n\\n<core_requirements>\\nYour solution MUST:\\n1. **Train a model** during the evaluation run (no pre-trained-only solutions)\\n2. **Generate predictions** for the test set in `/root/data/`\\n3. **Write predictions to `/workspace/submission.csv`** with the EXACT schema from `sample_submission.csv`\\n4. **Complete within {eval_timeout_hrs} hours** on 1× H200 GPU, 120GB RAM, 12 CPUs\\n\\n</core_requirements>\\n\\n<environment>\\n**Execution Model:**\\n- Each tool call starts fresh; only filesystem persists between calls\\n- Individual tool calls timeout after {timeout_min} minutes\\n- Total context window: {context_size} tokens\\n- Maximum interaction turns: {max_turns}\\n\\n**Available Resources:**\\n- Network: Enabled for pip/mamba installs and downloading models/data\\n- Pre-installed: Common ML libraries (torch, transformers, sklearn, etc.)\\n- Data location: `/root/data/` (read-only, contains train & test sets)\\n\\n**Critical Limits:**\\n- Streaming output: MAX 50 blocks (see troubleshooting below)\\n- Memory: 120GB RAM, 80GB GPU memory\\n- Time: {eval_timeout_hrs} hours total runtime\\n\\n</environment>\\n\\n\\n<important_lesson>\\n{important_lesson}\\n</important_lesson>\\n\\n\\n<final_submission>\\n\\nWhen you call the submit tool, provide:\\n\\n<summary>\\nA 3-5 sentence description of your approach:\\n- Data preprocessing steps\\n- Model architecture and key hyperparameters\\n- Training strategy\\n- Any notable techniques or external data used\\n</summary>\\n\\n<code>\\nYour complete, self-contained Python script that:\\n1. Loads data from `/root/data/`\\n2. Trains a model (during evaluation run)\\n3. Generates predictions\\n4. Writes `/workspace/submission.csv` with correct schema\\n5. Completes within {eval_timeout_hrs} hours\\n\\nThe script must be runnable as: `python solution.py`\\n</code>\\n\\n\\n**Verification Checklist Before Submitting:**\\n- [ ] Writes to `/workspace/submission.csv` (exact path)\\n- [ ] Row count, column names, and IDs match sample_submission EXACTLY\\n- [ ] Used `df.to_csv(..., index=False)` - no manual CSV string formatting\\n- [ ] No duplicate headers in outputs submission.csv file\\n- [ ] Tested end-to-end on subset\\n- [ ] Called `submit /workspace/solution.py`\\n</final_submission>\\n\\n<final_instructions>\\nDevelop incrementally. Create a working baseline first, then enhance. Document your reasoning at key decision points. Submit only when confident the solution runs end-to-end within constraints.\\n</final_instructions>\\n\"\"\".strip()\\'\\'\\'\\n\\nprint(f\"Loaded {len(rollout_df)} rollouts, {len(codebase)} codebase files, config YAML, and prompt\")\\n'}\n",
      "other_backends: None\n"
     ]
    }
   ],
   "source": [
    "# View metadata\n",
    "print(\"=== METADATA ===\")\n",
    "for k, v in metadata.items():\n",
    "    if k != \"backend_kwargs\":\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ba91dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp-based runtime: 246.17s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compare with timestamp-based runtime\n",
    "runtime = get_total_runtime(entries)\n",
    "print(f\"Timestamp-based runtime: {runtime.total_seconds():.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6da6ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd93dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations taken: 6\n",
      "Number of iterations (from extract_all): 6\n"
     ]
    }
   ],
   "source": [
    "# Check number of iterations actually taken by model\n",
    "num_iterations = len(iterations)\n",
    "print(f\"Number of iterations taken: {num_iterations}\")\n",
    "\n",
    "# You can also use extract_all for a comprehensive summary\n",
    "summary = extract_all(LOG_PATH)\n",
    "print(f\"Number of iterations (from extract_all): {summary['num_iterations']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5627fc3",
   "metadata": {},
   "source": [
    "# Final Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2e39a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL ANSWER ===\n",
      "<error_summary>\n",
      "The most common evaluation failure is 'SystemExit: 2', occurring in 1669 invalid submissions; scripts are exiting before training/inference runs.\n",
      "Top 5 error messages and counts:\n",
      "- 'SystemExit: 2': 1669\n",
      "- 'Validation error: Submission invalid! The attempt to grade the submission has resulted in the following error message:': 83\n",
      "- (empty message): 23\n",
      "- 'ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.': 16\n",
      "- 'RuntimeError: Encountered NaN.': 11\n",
      "</error_summary>\n",
      "\n",
      "<root_cause>\n",
      "User scripts are executed under a Jupyter/IPython runner (see config eval_execution_mode: jupyter), which injects extra CLI args (e.g., -f <json>) into sys.argv. Many solutions call argparse.parse_args() unguarded; argparse treats the injected args as unknown and terminates with SystemExit(2). In our data, 1669/1669 of 'SystemExit: 2' submissions included argparse usage, indicating the failure stems from unguarded CLI parsing rather than a grading bug.\n",
      "</root_cause>\n",
      "\n",
      "<important_lesson>\n",
      "You are executed under a Jupyter/IPython runner, which injects extra CLI args (e.g., -f <json>) into sys.argv. To avoid SystemExit: 2 and ensure your solution runs end-to-end:\n",
      "\n",
      "- Your script must run as: python solution.py with zero required flags.\n",
      "- If you use argparse, guard the entrypoint and ignore unknown IPython args.\n",
      "- Prefer parse_known_args() under IPython/Jupyter, or strip extraneous argv.\n",
      "- Do not call sys.exit(...) on normal paths; raise exceptions and handle them.\n",
      "- Always write /workspace/submission.csv via df.to_csv(..., index=False) and validate schema/row count against /root/data/sample_submission.csv.\n",
      "\n",
      "Drop-in robust entrypoint pattern:\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    import sys, argparse\n",
      "    IN_IPY = (\"ipykernel\" in sys.modules) or (\"IPython\" in sys.modules)\n",
      "\n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument(\"--fast_dev\", action=\"store_true\", help=\"Quick validation run\")\n",
      "\n",
      "    # Avoid SystemExit: 2 from unknown IPython args (e.g., '-f', '<json>')\n",
      "    if IN_IPY:\n",
      "        args, unknown = parser.parse_known_args()\n",
      "        if unknown:\n",
      "            print(f\"[warn] Ignoring unknown args from environment: {unknown}\")\n",
      "    else:\n",
      "        args = parser.parse_args()\n",
      "\n",
      "    main(fast_dev=getattr(args, \"fast_dev\", False))\n",
      "\n",
      "Schema/row-count check before finishing:\n",
      "\n",
      "import pandas as pd\n",
      "ss = pd.read_csv(\"/root/data/sample_submission.csv\")\n",
      "sub = pd.read_csv(\"/workspace/submission.csv\")\n",
      "assert list(sub.columns) == list(ss.columns), f\"Columns mismatch: {sub.columns} vs {ss.columns}\"\n",
      "assert len(sub) == len(ss), f\"Row count mismatch: {len(sub)} vs {len(ss)}\"\n",
      "</important_lesson>\n"
     ]
    }
   ],
   "source": [
    "# Get the final answer\n",
    "final_answer = get_final_answer(iterations)\n",
    "print(\"=== FINAL ANSWER ===\")\n",
    "print(final_answer if final_answer else \"No final answer found\")\n",
    "# print(f\"\\n(Total length: {len(final_answer) if final_answer else 0} chars)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050a9b29",
   "metadata": {},
   "source": [
    "# Rrompt Analysis\n",
    "\n",
    "```\n",
    "tell me what is in the prompt here:\n",
    "\n",
    "/checkpoint/maui_sft/winnieyangwn/rlm_dumps/gpt-5_common_invalid_errors_codebase_prompt_complete_513_2026-02-03_22-24-17_prompt.py\n",
    "\n",
    "does it include argparse related improvement?\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9bceda23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary of all iterations ===\n",
      "Iteration 1 [✗]: 1 code blocks\n",
      "  - Block 0: ModuleNotFoundError: No module named 'pandas'\n",
      "Iteration 2 [✓]: 1 code blocks\n",
      "Iteration 3 [✗]: 1 code blocks\n",
      "  - Block 0: NameError: name 'rollout_df' is not defined\n",
      "Iteration 4 [✗]: 1 code blocks\n",
      "  - Block 0: TypeError: 'NoneType' object is not callable\n",
      "Iteration 5 [✓]: 2 code blocks\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check all iterations for errors\n",
    "print(\"=== Summary of all iterations ===\")\n",
    "for it in iterations:\n",
    "    iter_num = it.get(\"iteration\", 0)\n",
    "    code_blocks = it.get(\"code_blocks\", [])\n",
    "    errors = []\n",
    "    for i, block in enumerate(code_blocks):\n",
    "        stderr = block.get(\"result\", {}).get(\"stderr\", \"\")\n",
    "        if stderr:\n",
    "            # Extract just the error type and message\n",
    "            error_line = stderr.strip().split('\\n')[-1] if stderr.strip() else \"\"\n",
    "            errors.append(f\"Block {i}: {error_line[:80]}\")\n",
    "    \n",
    "    status = \"✓\" if not errors else \"✗\"\n",
    "    print(f\"Iteration {iter_num} [{status}]: {len(code_blocks)} code blocks\")\n",
    "    for err in errors:\n",
    "        print(f\"  - {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8d29b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
