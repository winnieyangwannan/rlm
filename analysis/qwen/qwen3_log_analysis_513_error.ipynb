{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66628fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "\n",
    "\n",
    "sys.path.append('..')\n",
    "import rlm_log_utils\n",
    "importlib.reload(rlm_log_utils)\n",
    "from rlm_log_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94b4e70",
   "metadata": {},
   "source": [
    "## Usage Example\n",
    "\n",
    "Load the log file and extract key information:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1754fc72",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f328b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16 iterations\n"
     ]
    }
   ],
   "source": [
    "LOG_PATH = \"/checkpoint/maui_sft/winnieyangwn/rlm_dumps/cwm_common_invalid_errors_codebase_513_2026-02-05_07-24-06_0fd8b8aa.jsonl\"\n",
    "# Load the log - first entry is metadata, rest are iterations\n",
    "entries = load_rlm_log(LOG_PATH)\n",
    "metadata = entries[0]\n",
    "iterations = entries[1:]\n",
    "\n",
    "print(f\"Loaded {len(iterations)} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a88249",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2f17c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== METADATA ===\n",
      "type: metadata\n",
      "timestamp: 2026-02-05T07:24:06.308003\n",
      "root_model: facebook/cwm-sft\n",
      "max_depth: 2\n",
      "max_iterations: 100\n",
      "backend: vllm\n",
      "environment_type: local\n",
      "environment_kwargs: {'setup_code': '\\nimport pandas as pd\\nfrom pathlib import Path\\n\\n# Load rollout data as DataFrame\\nrollout_df = pd.read_json(\\'/checkpoint/maui_sft/winnieyangwn/amaia_dumps/513/trajectories/513_metadata.jsonl\\', lines=True)\\n\\n# Load codebase into dict\\ncodebase = {}\\ncodebase_root = Path(\\'/checkpoint/agentic-models/winnieyangwn/amaia_dumps/503/code/2026_02_02_00_55_44\\')\\nfor ext in [\\'.py\\', \\'.md\\', \\'.yaml\\']:\\n    for path in codebase_root.rglob(f\\'*{ext}\\'):\\n        try:\\n            rel_path = str(path.relative_to(codebase_root))\\n            codebase[rel_path] = path.read_text(errors=\\'ignore\\')\\n        except Exception:\\n            pass  # Skip unreadable files\\n\\n# Load config YAML\\nconfig_yaml = \\'\\'\\'# gpt5 after rate limit fix + using jupyter eval as baseline\\n# python -m launchers.stool run name=\"513\" script=apps.sea.eval config=apps/sea/configs/winnieyang/eval/baseline/gpt5/513.yaml nodes=3 group=maui_sft qos=h200_maui_sft_high dirs_exists_ok=True\\n\\n\\n\\n# Output directory for evaluation results\\ndump_dir: /checkpoint/maui_sft/winnieyangwn/amaia_dumps/513\\n# Generation backend configuration\\ngen_backend: litellm\\n\\nlitellm_args:\\n  model: azure/gpt-5\\n  api_key: \"6524db61b4774663a00ba80558122ceb\"\\n  base_url: https://azure-services-fair-openai1-eastus2n3.azure-api.net/openai/deployments/gpt-5\\n  api_version: 2025-03-01-preview\\n  tools_env: mle_bench_bash_env\\n  max_retries: 64\\n  retry_min_wait: 4.0\\n  # retry_max_wait: 60.0\\n\\ngen_args:\\n  use_sampling: true\\n  temperature: 1.0\\n  top_p: 0.95\\n  max_batch: 8\\n  max_gen: 32768\\n\\n\\n# Tokenizer configuration (matching the training config)\\ntokenizer:\\n  name: cwm_instruct\\n  path: /engshare/jcaudet/amaia_checkpoints/cwm/tokenizer.model\\n\\n# Evaluation tasks configuration - using the new mle_bench_bash environment\\ntasks:\\n  - env_config: mle_bench_bash\\n    reward_fn: mle_bench\\n    path: /checkpoint/maui_sft/shared/kniu/datasets/mlebench_full.jsonl  \\n    samples_per_prompt: 64\\n    init_args:\\n      config:\\n        model: \"gpt5\"\\n        prompt_file: \"gpt5-513\"\\n        think: false\\n        context_size: 98304\\n        max_turns: 128\\n        max_action_len: 16384\\n        backend: agentbox\\n        agentbox_manager_uri:  h200-137-003-080:46725 # CHANGE: Need to update every run\\n        session_timeout: 1200.0\\n        eval_timeout: 32400\\n        eval_execution_mode: jupyter  # Options: bash, jupyter - controls how solution.py is executed during evaluation\\n        use_think_tag: false\\n        training: false\\n        benchmark: mlebench\\n    metrics_spec:\\n      pass:\\n        - \"@1\"\\n      execution_outcome:\\n        - \"@1\"\\n\\n# Runtime configuration\\nnum_rollout_threads: 8  # CHANGE: based on num workers an AgentBox server size\\ndata_queue_size: 10000\\nseed: 42\\nperf_log_freq: 60.0\\n\\n# Dump configuration\\ndump_mode: minimal  # Options: full, minimal, none\\ndump_compress: false\\nkeep_start_args: [\"q0_monte_carlo\", \"s0\", \"task_id\", \"instance_id\"]\\nrun_metrics_aggregation: true\\n\\n# Logging configuration\\nlogging:\\n  enable_tensorboard: false\\n  enable_wandb: false\\n  wandb:\\n    entity: none\\n    project: none\\n    name: none\\n\\n# System configuration\\nsetup:\\n  spawn_method: forkserver\\n  torch_init_timeout: 600\\n  cuda_matmul_allow_tf32: true\\n\\nlog_level: debug\\nmax_exceptions: 3\\'\\'\\'\\n\\nprint(f\"Loaded {len(rollout_df)} rollouts, {len(codebase)} codebase files, and config YAML\")\\n'}\n",
      "other_backends: None\n"
     ]
    }
   ],
   "source": [
    "# View metadata\n",
    "print(\"=== METADATA ===\")\n",
    "for k, v in metadata.items():\n",
    "    if k != \"backend_kwargs\":\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba91dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp-based runtime: 44.93s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compare with timestamp-based runtime\n",
    "runtime = get_total_runtime(entries)\n",
    "print(f\"Timestamp-based runtime: {runtime.total_seconds():.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5627fc3",
   "metadata": {},
   "source": [
    "# Final Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2e39a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL ANSWER ===\n",
      "Error: Variable 'final_answer' not found\n"
     ]
    }
   ],
   "source": [
    "# Get the final answer\n",
    "final_answer = get_final_answer(iterations)\n",
    "print(\"=== FINAL ANSWER ===\")\n",
    "print(final_answer if final_answer else \"No final answer found\")\n",
    "# print(f\"\\n(Total length: {len(final_answer) if final_answer else 0} chars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7dd0c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Last Iteration (#16) ===\n",
      "Keys in last iteration: ['type', 'iteration', 'timestamp', 'prompt', 'response', 'code_blocks', 'final_answer', 'iteration_time']\n",
      "\n",
      "Final answer in last iteration: Error: Variable 'final_answer' not found\n",
      "\n",
      "Number of code blocks: 4\n",
      "\n",
      "--- Code Block 0 ---\n",
      "Code:\n",
      "# Get invalid submissions\n",
      "invalid_submissions = rollout_df[~rollout_df[\"valid_submission\"]]\n",
      "\n",
      "# Count the occurrence of each eval_error_output\n",
      "error_counts = invalid_submissions[\"eval_error_output\"].value_counts()\n",
      "\n",
      "# Get the top 5 most common eval_error_output messages\n",
      "top_error_messages = error_counts[:5]\n",
      "\n",
      "# Print the top 5 most common eval_error_output messages\n",
      "print(top_error_messages)\n",
      "\n",
      "Stdout:\n",
      "eval_error_output\n",
      "SystemExit: 2\\nAn exception has occurred, use %tb to see the full traceback.\\n\\nSystemExit: 2\\n                                                                                                                                                                                                                                                                                                                                                                                                   ...\n",
      "\n",
      "--- Code Block 1 ---\n",
      "Code:\n",
      "# Get the most common eval_error_output message\n",
      "top_error_message = top_error_messages.index[0]\n",
      "\n",
      "# Print the most common eval_error_output message\n",
      "print(top_error_message)\n",
      "\n",
      "# Find the rows in rollout_df with the most common eval_error_output message\n",
      "top_error_rows = rollout_df[rollout_df[\"eval_error_output\"] == top_error_message]\n",
      "\n",
      "# Print the submission code for these rows\n",
      "print(top_error_rows[\"code\"].head(5))  # Show 5 example code solutions\n",
      "\n",
      "Stdout:\n",
      "SystemExit: 2\n",
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\n",
      "SystemExit: 2\n",
      "\n",
      "0    #!/usr/bin/env python3\\nimport os\\nimport sys\\...\n",
      "1    #!/usr/bin/env python3\\nimport os\\nimport sys\\...\n",
      "2    import os\\nimport sys\\nimport math\\nimport csv...\n",
      "3    #!/usr/bin/env python3\\nimport os\\nimport sys\\...\n",
      "4    #!/usr/bin/env python3\\nimport os\\nimport sys\\...\n",
      "Name: code, dtype: str\n",
      "\n",
      "\n",
      "--- Code Block 2 ---\n",
      "Code:\n",
      "# Get the most common eval_error_output message\n",
      "top_error_message = top_error_messages.index[0]\n",
      "\n",
      "# Search for relevant files in the CODEBASE\n",
      "relevant_files = [file for file, code in codebase.items() if top_error_message in code]\n",
      "\n",
      "# Print the relevant files\n",
      "print(relevant_files[:5])  # Show 5 relevant files\n",
      "\n",
      "Stdout:\n",
      "[]\n",
      "\n",
      "\n",
      "--- Code Block 3 ---\n",
      "Code:\n",
      "# Get the most common eval_error_output message\n",
      "top_error_message = top_error_messages.index[0]\n",
      "\n",
      "# Print the root cause analysis\n",
      "print(\"The most common evaluation error is due to\")\n",
      "\n",
      "# Print the likely root cause of this error\n",
      "print(\"The likely root cause of this error is\")\n",
      "\n",
      "# Print the specific code references\n",
      "print(\"The specific code references are\")\n",
      "\n",
      "# Print the proposed fixes\n",
      "print(\"The proposed fixes are\")\n",
      "\n",
      "Stdout:\n",
      "The most common evaluation error is due to\n",
      "The likely root cause of this error is\n",
      "The specific code references are\n",
      "The proposed fixes are\n",
      "\n",
      "\n",
      "=== LLM Response in Last Iteration ===\n",
      "Response (last 1000 chars):\n",
      "...traceback to get more detailed error information. \n",
      "\n",
      "Variable name: final_answer\n",
      "\n",
      "```python\n",
      "# Save the final answer to a variable\n",
      "final_answer = \"The most common evaluation error is due to exceptions being raised during the execution of the solution script. The likely root cause of this error is due to the fact that the solution script is becoming too large and exceeding frame chunking limits, leading to deserialization errors. To fix this issue, the code could be refactored to handle large outputs and exceptions more robustly, and the sampling process could be adjusted to use the user's prompt todownsample the early parts of rollouts with error outputs, reducing over-theme repetition. Additionally, the solution script execution could be performed in a separate process or container using async or run methods, and the eval error output could be changed to be a JSON with error type and traceback to get more detailed error information.\"\n",
      "\n",
      "# Print the final answer\n",
      "FINAL_VAR(final_answer)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check the last iteration's response and code blocks to understand why final_answer is not found\n",
    "last_iteration = iterations[-1] if iterations else None\n",
    "\n",
    "if last_iteration:\n",
    "    print(f\"=== Last Iteration (#{last_iteration.get('iteration', 'N/A')}) ===\")\n",
    "    print(f\"Keys in last iteration: {list(last_iteration.keys())}\")\n",
    "    print(f\"\\nFinal answer in last iteration: {last_iteration.get('final_answer', 'NOT FOUND')}\")\n",
    "    \n",
    "    # Check code blocks in last iteration\n",
    "    code_blocks = last_iteration.get(\"code_blocks\", [])\n",
    "    print(f\"\\nNumber of code blocks: {len(code_blocks)}\")\n",
    "    \n",
    "    for i, block in enumerate(code_blocks):\n",
    "        print(f\"\\n--- Code Block {i} ---\")\n",
    "        code = block.get(\"code\", \"\")\n",
    "        print(f\"Code:\\n{code[:500]}...\" if len(code) > 500 else f\"Code:\\n{code}\")\n",
    "        \n",
    "        result = block.get(\"result\", {})\n",
    "        stdout = result.get(\"stdout\", \"\")\n",
    "        stderr = result.get(\"stderr\", \"\")\n",
    "        \n",
    "        if stdout:\n",
    "            print(f\"\\nStdout:\\n{stdout[:500]}...\" if len(stdout) > 500 else f\"\\nStdout:\\n{stdout}\")\n",
    "        if stderr:\n",
    "            print(f\"\\nStderr:\\n{stderr[:500]}...\" if len(stderr) > 500 else f\"\\nStderr:\\n{stderr}\")\n",
    "    \n",
    "    # Check LLM response to see if FINAL_VAR was called correctly\n",
    "    print(f\"\\n=== LLM Response in Last Iteration ===\")\n",
    "    response = last_iteration.get(\"response\", \"\")\n",
    "    print(f\"Response (last 1000 chars):\\n...{response[-1000:]}\" if len(response) > 1000 else f\"Response:\\n{response}\")\n",
    "else:\n",
    "    print(\"No iterations found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403cb227",
   "metadata": {},
   "source": [
    "## Analysis: Why `final_answer` shows \"Error: Variable 'final_answer' not found\"\n",
    "\n",
    "### Key Findings from Last Iteration (#16)\n",
    "\n",
    "Looking at the debug output, the problem is clear:\n",
    "\n",
    "### 1. The LLM wrote but didn't execute the final code block\n",
    "\n",
    "In the **LLM Response**, the model generated:\n",
    "```python\n",
    "final_answer = \"The most common evaluation error is...\"\n",
    "FINAL_VAR(final_answer)\n",
    "```\n",
    "\n",
    "However, this code block was **never executed**. The 4 code blocks that actually ran were:\n",
    "- **Code Block 0**: Gets error counts ✅ \n",
    "- **Code Block 1**: Prints top error message ✅\n",
    "- **Code Block 2**: Searches codebase (found nothing) ✅\n",
    "- **Code Block 3**: Just prints empty placeholder statements ✅\n",
    "\n",
    "### 2. The executed code never called `FINAL_VAR()`\n",
    "\n",
    "Code Block 3 only printed incomplete placeholder text:\n",
    "```python\n",
    "print(\"The most common evaluation error is due to\")\n",
    "print(\"The likely root cause of this error is\")\n",
    "# ... (no actual content, no FINAL_VAR call)\n",
    "```\n",
    "\n",
    "### 3. Root Cause\n",
    "\n",
    "The model generated the final answer in markdown/response text, but the code that would have set `final_answer` and called `FINAL_VAR(final_answer)` was **not parsed/executed** as a separate code block. The RLM system requires code to actually execute `FINAL_VAR(variable_name)` for the answer to be captured.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The LLM reached the iteration limit (16 iterations) without successfully executing a code block containing `FINAL_VAR()`. The final answer code was written in the response but either:\n",
    "1. The iteration ended before that code block could be executed\n",
    "2. The code block was malformed and not properly extracted for execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7608932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amaia-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
