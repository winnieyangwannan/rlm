{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1b0942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import zlib\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "import ast\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "\n",
    "sys.path.append('data/data_utils')\n",
    "import data_utils\n",
    "importlib.reload(data_utils)\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "320f4f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = 513\n",
    "file_path= 'f/checkpoint/maui_sft/winnieyangwn/amaia_dumps/{run_id}/trajectories/mle_bench_bashmle_bench_checkpoint_maui_sft_shared_kniu_datasets_mlebench_full_jsonl/mle_bench_bashmle_bench_checkpoint_maui_sft_shared_kniu_datasets_mlebench_full_jsonl.jsonl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98f86f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_767782/3726597042.py:1: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_trj = pd.read_json(file_path, lines=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected character found when decoding 'false'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_trj \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/home/winnieyangwn/miniforge3/envs/amaia-agent/lib/python3.11/site-packages/pandas/io/json/_json.py:815\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/home/winnieyangwn/miniforge3/envs/amaia-agent/lib/python3.11/site-packages/pandas/io/json/_json.py:1012\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         data \u001b[38;5;241m=\u001b[39m ensure_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m   1011\u001b[0m         data_lines \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1012\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_combine_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_lines\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1014\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m/storage/home/winnieyangwn/miniforge3/envs/amaia-agent/lib/python3.11/site-packages/pandas/io/json/_json.py:1040\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m   1038\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1040\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m/storage/home/winnieyangwn/miniforge3/envs/amaia-agent/lib/python3.11/site-packages/pandas/io/json/_json.py:1176\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1176\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/storage/home/winnieyangwn/miniforge3/envs/amaia-agent/lib/python3.11/site-packages/pandas/io/json/_json.py:1392\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1388\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1391\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[0;32m-> 1392\u001b[0m         \u001b[43mujson_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1393\u001b[0m     )\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1395\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1396\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[1;32m   1397\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m ujson_loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1398\u001b[0m     }\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpected character found when decoding 'false'"
     ]
    }
   ],
   "source": [
    "df_trj = pd.read_json(file_path, lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37360c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2526, 9)\n",
      "Columns: ['task_name', 'task_description', 'code', 'percentile', 'valid_submission', 'eval_error_message', 'eval_duration', 'rollout_duration', 'rollout']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>task_description</th>\n",
       "      <th>code</th>\n",
       "      <th>percentile</th>\n",
       "      <th>valid_submission</th>\n",
       "      <th>eval_error_message</th>\n",
       "      <th>eval_duration</th>\n",
       "      <th>rollout_duration</th>\n",
       "      <th>rollout</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aerial-cactus-identification</td>\n",
       "      <td># Overview\\n\\n## Overview\\n\\n### Description\\n...</td>\n",
       "      <td>#!/usr/bin/env python3\\nimport os\\nimport sys\\...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Execution returned non-zero exit code</td>\n",
       "      <td>0.0</td>\n",
       "      <td>186.208566</td>\n",
       "      <td>[{'turn_id': 0, 'action': '', 'observation': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aerial-cactus-identification</td>\n",
       "      <td># Overview\\n\\n## Overview\\n\\n### Description\\n...</td>\n",
       "      <td>#!/usr/bin/env python3\\nimport os\\nimport sys\\...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Execution returned non-zero exit code</td>\n",
       "      <td>0.0</td>\n",
       "      <td>188.979062</td>\n",
       "      <td>[{'turn_id': 0, 'action': '', 'observation': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>denoising-dirty-documents</td>\n",
       "      <td># Overview\\n\\n## Description\\n\\n[Optical Chara...</td>\n",
       "      <td>import os\\nimport sys\\nimport math\\nimport csv...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Execution returned non-zero exit code</td>\n",
       "      <td>0.0</td>\n",
       "      <td>194.721609</td>\n",
       "      <td>[{'turn_id': 0, 'action': '', 'observation': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>denoising-dirty-documents</td>\n",
       "      <td># Overview\\n\\n## Description\\n\\n[Optical Chara...</td>\n",
       "      <td>#!/usr/bin/env python3\\nimport os\\nimport sys\\...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Execution returned non-zero exit code</td>\n",
       "      <td>0.0</td>\n",
       "      <td>197.420535</td>\n",
       "      <td>[{'turn_id': 0, 'action': '', 'observation': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denoising-dirty-documents</td>\n",
       "      <td># Overview\\n\\n## Description\\n\\n[Optical Chara...</td>\n",
       "      <td>#!/usr/bin/env python3\\nimport os\\nimport sys\\...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Execution returned non-zero exit code</td>\n",
       "      <td>0.0</td>\n",
       "      <td>198.732197</td>\n",
       "      <td>[{'turn_id': 0, 'action': '', 'observation': '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      task_name  \\\n",
       "0  aerial-cactus-identification   \n",
       "1  aerial-cactus-identification   \n",
       "2     denoising-dirty-documents   \n",
       "3     denoising-dirty-documents   \n",
       "4     denoising-dirty-documents   \n",
       "\n",
       "                                    task_description  \\\n",
       "0  # Overview\\n\\n## Overview\\n\\n### Description\\n...   \n",
       "1  # Overview\\n\\n## Overview\\n\\n### Description\\n...   \n",
       "2  # Overview\\n\\n## Description\\n\\n[Optical Chara...   \n",
       "3  # Overview\\n\\n## Description\\n\\n[Optical Chara...   \n",
       "4  # Overview\\n\\n## Description\\n\\n[Optical Chara...   \n",
       "\n",
       "                                                code  percentile  \\\n",
       "0  #!/usr/bin/env python3\\nimport os\\nimport sys\\...         0.0   \n",
       "1  #!/usr/bin/env python3\\nimport os\\nimport sys\\...         0.0   \n",
       "2  import os\\nimport sys\\nimport math\\nimport csv...         0.0   \n",
       "3  #!/usr/bin/env python3\\nimport os\\nimport sys\\...         0.0   \n",
       "4  #!/usr/bin/env python3\\nimport os\\nimport sys\\...         0.0   \n",
       "\n",
       "   valid_submission                     eval_error_message  eval_duration  \\\n",
       "0             False  Execution returned non-zero exit code            0.0   \n",
       "1             False  Execution returned non-zero exit code            0.0   \n",
       "2             False  Execution returned non-zero exit code            0.0   \n",
       "3             False  Execution returned non-zero exit code            0.0   \n",
       "4             False  Execution returned non-zero exit code            0.0   \n",
       "\n",
       "   rollout_duration                                            rollout  \n",
       "0        186.208566  [{'turn_id': 0, 'action': '', 'observation': '...  \n",
       "1        188.979062  [{'turn_id': 0, 'action': '', 'observation': '...  \n",
       "2        194.721609  [{'turn_id': 0, 'action': '', 'observation': '...  \n",
       "3        197.420535  [{'turn_id': 0, 'action': '', 'observation': '...  \n",
       "4        198.732197  [{'turn_id': 0, 'action': '', 'observation': '...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Flatten the dataframe\n",
    "df_flat = flatten_dataframe(df_trj)\n",
    "\n",
    "print(f\"Shape: {df_flat.shape}\")\n",
    "print(f\"Columns: {df_flat.columns.tolist()}\")\n",
    "df_flat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b38af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186.20856591500342"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trj.iloc[0][\"rollouts\"][0][\"metrics\"][\"rollout/duration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd5e6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#!/usr/bin/env python3\\nimport os\\nimport sys\\nimport time\\nimport json\\nimport math\\nimport random\\nimport argparse\\nfrom dataclasses import dataclass\\n\\n# Attempt to import required packages; install if missing\\n\\ndef ensure_imports():\\n    import importlib\\n    need = []\\n    for pkg in [\\n        (\\'torch\\', \\'torch\\'),\\n        (\\'torchvision\\', \\'torchvision\\'),\\n        (\\'sklearn\\', \\'sklearn\\'),\\n        (\\'pandas\\', \\'pandas\\'),\\n        (\\'numpy\\', \\'numpy\\'),\\n        (\\'PIL\\', \\'PIL\\'),\\n    ]:\\n        try:\\n            importlib.import_module(pkg[0])\\n        except Exception:\\n            need.append(pkg[1])\\n    if need:\\n        print(f\"[setup] Installing missing packages: {need}\")\\n        import subprocess\\n        subprocess.check_call([sys.executable, \\'-m\\', \\'pip\\', \\'install\\', \\'--no-input\\'] + need)\\n\\nensure_imports()\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom PIL import Image\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom torch.utils.data import Dataset, DataLoader\\nimport torchvision.transforms as T\\nimport torchvision.models as models\\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\\nfrom sklearn.metrics import roc_auc_score\\n\\n# Reproducibility\\n\\ndef set_seed(seed=42):\\n    random.seed(seed)\\n    np.random.seed(seed)\\n    torch.manual_seed(seed)\\n    torch.cuda.manual_seed_all(seed)\\n    torch.backends.cudnn.deterministic = False\\n    torch.backends.cudnn.benchmark = True\\n\\n# Dataset\\n\\nclass CactusDataset(Dataset):\\n    def __init__(self, df, img_dir, transform=None, is_test=False):\\n        self.df = df.reset_index(drop=True)\\n        self.img_dir = img_dir\\n        self.transform = transform\\n        self.is_test = is_test\\n    def __len__(self):\\n        return len(self.df)\\n    def __getitem__(self, idx):\\n        row = self.df.iloc[idx]\\n        img_path = os.path.join(self.img_dir, row[\\'id\\'])\\n        with Image.open(img_path) as im:\\n            im = im.convert(\\'RGB\\')\\n            if self.transform:\\n                im = self.transform(im)\\n        if self.is_test:\\n            return im, row[\\'id\\']\\n        label = float(row[\\'has_cactus\\'])\\n        return im, torch.tensor([label], dtype=torch.float32)\\n\\n# Model builder\\n\\ndef build_model(num_classes=1):\\n    # Use ResNet18 pretrained, adapt to binary output\\n    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\\n    in_features = model.fc.in_features\\n    model.fc = nn.Linear(in_features, num_classes)\\n    return model\\n\\n# Train and evaluate\\n\\n@dataclass\\nclass TrainConfig:\\n    img_size: int = 96\\n    batch_size: int = 256\\n    epochs: int = 10\\n    lr: float = 1e-3\\n    weight_decay: float = 1e-4\\n    val_split: float = 0.1\\n    num_workers: int = 4\\n    amp: bool = True\\n    patience: int = 3\\n\\n\\ndef get_transforms(img_size):\\n    mean = [0.485, 0.456, 0.406]\\n    std = [0.229, 0.224, 0.225]\\n    train_tf = T.Compose([\\n        T.Resize((img_size, img_size)),\\n        T.RandomHorizontalFlip(),\\n        T.RandomVerticalFlip(),\\n        T.RandomRotation(20),\\n        T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\\n        T.ToTensor(),\\n        T.Normalize(mean, std),\\n    ])\\n    val_tf = T.Compose([\\n        T.Resize((img_size, img_size)),\\n        T.ToTensor(),\\n        T.Normalize(mean, std),\\n    ])\\n    return train_tf, val_tf\\n\\n\\ndef train_one_epoch(model, loader, optimizer, scaler, device):\\n    model.train()\\n    loss_meter = 0.0\\n    n = 0\\n    for images, targets in loader:\\n        images = images.to(device)\\n        targets = targets.to(device)\\n        optimizer.zero_grad(set_to_none=True)\\n        if scaler is not None:\\n            with torch.cuda.amp.autocast():\\n                logits = model(images)\\n                loss = F.binary_cross_entropy_with_logits(logits.view(-1), targets.view(-1))\\n            scaler.scale(loss).backward()\\n            scaler.step(optimizer)\\n            scaler.update()\\n        else:\\n            logits = model(images)\\n            loss = F.binary_cross_entropy_with_logits(logits.view(-1), targets.view(-1))\\n            loss.backward()\\n            optimizer.step()\\n        batch = images.size(0)\\n        loss_meter += loss.item() * batch\\n        n += batch\\n    return loss_meter / max(n, 1)\\n\\n\\ndef evaluate(model, loader, device):\\n    model.eval()\\n    loss_meter = 0.0\\n    n = 0\\n    all_probs = []\\n    all_targets = []\\n    with torch.no_grad():\\n        for images, targets in loader:\\n            images = images.to(device)\\n            targets = targets.to(device)\\n            logits = model(images)\\n            loss = F.binary_cross_entropy_with_logits(logits.view(-1), targets.view(-1))\\n            probs = torch.sigmoid(logits.view(-1))\\n            all_probs.append(probs.cpu().numpy())\\n            all_targets.append(targets.view(-1).cpu().numpy())\\n            batch = images.size(0)\\n            loss_meter += loss.item() * batch\\n            n += batch\\n    all_probs = np.concatenate(all_probs) if all_probs else np.array([])\\n    all_targets = np.concatenate(all_targets) if all_targets else np.array([])\\n    auc = roc_auc_score(all_targets, all_probs) if len(all_targets) > 0 and len(np.unique(all_targets)) > 1 else float(\\'nan\\')\\n    return loss_meter / max(n, 1), auc\\n\\n\\ndef train_model(train_df, cfg: TrainConfig, data_root, fast_dev=False):\\n    device = torch.device(\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\')\\n    print(f\"[info] Using device: {device}\")\\n    train_tf, val_tf = get_transforms(cfg.img_size)\\n\\n    # Split train/val\\n    train_df_, val_df_ = train_test_split(train_df, test_size=cfg.val_split, random_state=42, stratify=train_df[\\'has_cactus\\'])\\n\\n    if fast_dev:\\n        # Use small subset for quick validation\\n        train_df_ = train_df_.sample(n=min(1024, len(train_df_)), random_state=42)\\n        val_df_ = val_df_.sample(n=min(256, len(val_df_)), random_state=42)\\n        print(f\"[dev] Using subset: train {len(train_df_)} val {len(val_df_)}\")\\n\\n    train_ds = CactusDataset(train_df_, img_dir=os.path.join(data_root, \\'train\\'), transform=train_tf, is_test=False)\\n    val_ds = CactusDataset(val_df_, img_dir=os.path.join(data_root, \\'train\\'), transform=val_tf, is_test=False)\\n\\n    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers, pin_memory=True)\\n    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\\n\\n    model = build_model(num_classes=1)\\n    model.to(device)\\n\\n    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epochs)\\n    scaler = torch.cuda.amp.GradScaler() if (cfg.amp and device.type == \\'cuda\\') else None\\n\\n    best_auc = -1.0\\n    best_epoch = -1\\n    best_path = \\'/workspace/best_model.pth\\'\\n    es_counter = 0\\n\\n    for epoch in range(cfg.epochs):\\n        t0 = time.time()\\n        train_loss = train_one_epoch(model, train_loader, optimizer, scaler, device)\\n        val_loss, val_auc = evaluate(model, val_loader, device)\\n        scheduler.step()\\n        dt = time.time() - t0\\n        print(f\"[epoch {epoch+1}/{cfg.epochs}] train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_auc={val_auc:.4f} time={dt:.1f}s\")\\n        if not math.isnan(val_auc) and val_auc > best_auc:\\n            best_auc = val_auc\\n            best_epoch = epoch\\n            torch.save({\\'model\\': model.state_dict(), \\'cfg\\': cfg.__dict__}, best_path)\\n            es_counter = 0\\n            print(f\"[checkpoint] Saved best model with AUC={best_auc:.4f} at epoch {epoch+1}\")\\n        else:\\n            es_counter += 1\\n        if es_counter >= cfg.patience:\\n            print(f\"[early_stop] No improvement for {cfg.patience} epochs. Stopping.\")\\n            break\\n\\n    # Load best\\n    if os.path.exists(best_path):\\n        ckpt = torch.load(best_path, map_location=device)\\n        model.load_state_dict(ckpt[\\'model\\'])\\n        print(f\"[load] Loaded best checkpoint from epoch {best_epoch+1} with AUC={best_auc:.4f}\")\\n    else:\\n        print(\"[warn] Best checkpoint not found; using current model\")\\n    return model, val_loader\\n\\n\\ndef infer(model, test_df, cfg: TrainConfig, data_root):\\n    device = torch.device(\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\')\\n    _, val_tf = get_transforms(cfg.img_size)\\n    test_ds = CactusDataset(test_df, img_dir=os.path.join(data_root, \\'test\\'), transform=val_tf, is_test=True)\\n    test_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\\n    model.eval()\\n    model.to(device)\\n    ids = []\\n    probs = []\\n    with torch.no_grad():\\n        for images, img_ids in test_loader:\\n            images = images.to(device)\\n            logits = model(images)\\n            p = torch.sigmoid(logits.view(-1)).cpu().numpy()\\n            ids.extend(list(img_ids))\\n            probs.extend(list(p))\\n    return ids, probs\\n\\n\\ndef main():\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\\'--data_root\\', type=str, default=\\'/root/data\\')\\n    parser.add_argument(\\'--output\\', type=str, default=\\'/workspace/submission.csv\\')\\n    parser.add_argument(\\'--fast_dev\\', action=\\'store_true\\')\\n    args = parser.parse_args()\\n\\n    set_seed(42)\\n\\n    # Load CSVs\\n    train_csv = os.path.join(args.data_root, \\'train.csv\\')\\n    sample_csv = os.path.join(args.data_root, \\'sample_submission.csv\\')\\n    train_df = pd.read_csv(train_csv)\\n    # Ensure correct dtypes\\n    train_df[\\'has_cactus\\'] = train_df[\\'has_cactus\\'].astype(int)\\n\\n    # Decide training config\\n    fast_dev_env = os.environ.get(\\'FAST_DEV_RUN\\', \\'0\\') == \\'1\\'\\n    fast_mode = args.fast_dev or fast_dev_env\\n    cfg = TrainConfig()\\n    if fast_mode:\\n        cfg.epochs = 2\\n        cfg.batch_size = 128\\n        cfg.num_workers = 2\\n        print(\\'[mode] FAST_DEV_RUN enabled for quick validation\\')\\n\\n    # Train\\n    model, val_loader = train_model(train_df, cfg, args.data_root, fast_dev=fast_mode)\\n\\n    # Inference\\n    # Respect sample submission order if available\\n    if os.path.exists(sample_csv):\\n        test_df = pd.read_csv(sample_csv)\\n        test_df = test_df[[\\'id\\']]\\n        print(f\"[info] Using sample_submission order with {len(test_df)} rows\")\\n    else:\\n        # List test dir\\n        test_dir = os.path.join(args.data_root, \\'test\\')\\n        ids = sorted([f for f in os.listdir(test_dir) if f.lower().endswith(\\'.jpg\\')])\\n        test_df = pd.DataFrame({\\'id\\': ids})\\n        print(f\"[info] Using directory listing order with {len(test_df)} rows\")\\n\\n    ids, probs = infer(model, test_df, cfg, args.data_root)\\n\\n    # Write submission\\n    os.makedirs(os.path.dirname(args.output), exist_ok=True)\\n    sub_df = pd.DataFrame({\\'id\\': ids, \\'has_cactus\\': probs})\\n    sub_df.to_csv(args.output, index=False)\\n    print(f\"[done] Wrote submission to {args.output} with {len(sub_df)} rows\")\\n\\n\\nif __name__ == \\'__main__\\':\\n    main()\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trj.iloc[0][\"rollouts\"][0][\"traj\"][\"transitions\"][-1][\"info\"][\"pred_solution\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0df43fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rollout(df_trj, rollout_id: int) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Build a rollout as a list of turn dicts for a given rollout_id.\n",
    "    \n",
    "    Args:\n",
    "        df_trj: DataFrame containing trajectory data\n",
    "        rollout_id: Index of the rollout in the DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        List of dicts, each with \"action\" and \"observation\" fields\n",
    "    \"\"\"\n",
    "    transitions = df_trj.iloc[rollout_id][\"rollouts\"][0][\"traj\"][\"transitions\"]\n",
    "    \n",
    "    rollout = []\n",
    "    for t, turn in enumerate(transitions):\n",
    "        rollout.append({\n",
    "            \"turn_id\": t,\n",
    "            \"action\": turn[\"action_str\"],\n",
    "            \"observation\": turn[\"observation_str\"]\n",
    "        })\n",
    "    \n",
    "    return rollout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19290d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2526, 9)\n",
      "Columns: ['task_name', 'task_description', 'code', 'percentile', 'valid_submission', 'eval_error_message', 'eval_duration', 'rollout_duration', 'rollout']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>task_description</th>\n",
       "      <th>code</th>\n",
       "      <th>percentile</th>\n",
       "      <th>valid_submission</th>\n",
       "      <th>eval_error_message</th>\n",
       "      <th>eval_duration</th>\n",
       "      <th>rollout_duration</th>\n",
       "      <th>rollout</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aerial-cactus-identification</td>\n",
       "      <td># Overview\\n\\n## Overview\\n\\n### Description\\n...</td>\n",
       "      <td>#!/usr/bin/env python3\\nimport os\\nimport sys\\...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Execution returned non-zero exit code</td>\n",
       "      <td>0.0</td>\n",
       "      <td>186.208566</td>\n",
       "      <td>[{'turn_id': 0, 'action': '', 'observation': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aerial-cactus-identification</td>\n",
       "      <td># Overview\\n\\n## Overview\\n\\n### Description\\n...</td>\n",
       "      <td>#!/usr/bin/env python3\\nimport os\\nimport sys\\...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Execution returned non-zero exit code</td>\n",
       "      <td>0.0</td>\n",
       "      <td>188.979062</td>\n",
       "      <td>[{'turn_id': 0, 'action': '', 'observation': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>denoising-dirty-documents</td>\n",
       "      <td># Overview\\n\\n## Description\\n\\n[Optical Chara...</td>\n",
       "      <td>import os\\nimport sys\\nimport math\\nimport csv...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Execution returned non-zero exit code</td>\n",
       "      <td>0.0</td>\n",
       "      <td>194.721609</td>\n",
       "      <td>[{'turn_id': 0, 'action': '', 'observation': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>denoising-dirty-documents</td>\n",
       "      <td># Overview\\n\\n## Description\\n\\n[Optical Chara...</td>\n",
       "      <td>#!/usr/bin/env python3\\nimport os\\nimport sys\\...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Execution returned non-zero exit code</td>\n",
       "      <td>0.0</td>\n",
       "      <td>197.420535</td>\n",
       "      <td>[{'turn_id': 0, 'action': '', 'observation': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denoising-dirty-documents</td>\n",
       "      <td># Overview\\n\\n## Description\\n\\n[Optical Chara...</td>\n",
       "      <td>#!/usr/bin/env python3\\nimport os\\nimport sys\\...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Execution returned non-zero exit code</td>\n",
       "      <td>0.0</td>\n",
       "      <td>198.732197</td>\n",
       "      <td>[{'turn_id': 0, 'action': '', 'observation': '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      task_name  \\\n",
       "0  aerial-cactus-identification   \n",
       "1  aerial-cactus-identification   \n",
       "2     denoising-dirty-documents   \n",
       "3     denoising-dirty-documents   \n",
       "4     denoising-dirty-documents   \n",
       "\n",
       "                                    task_description  \\\n",
       "0  # Overview\\n\\n## Overview\\n\\n### Description\\n...   \n",
       "1  # Overview\\n\\n## Overview\\n\\n### Description\\n...   \n",
       "2  # Overview\\n\\n## Description\\n\\n[Optical Chara...   \n",
       "3  # Overview\\n\\n## Description\\n\\n[Optical Chara...   \n",
       "4  # Overview\\n\\n## Description\\n\\n[Optical Chara...   \n",
       "\n",
       "                                                code  percentile  \\\n",
       "0  #!/usr/bin/env python3\\nimport os\\nimport sys\\...         0.0   \n",
       "1  #!/usr/bin/env python3\\nimport os\\nimport sys\\...         0.0   \n",
       "2  import os\\nimport sys\\nimport math\\nimport csv...         0.0   \n",
       "3  #!/usr/bin/env python3\\nimport os\\nimport sys\\...         0.0   \n",
       "4  #!/usr/bin/env python3\\nimport os\\nimport sys\\...         0.0   \n",
       "\n",
       "   valid_submission                     eval_error_message  eval_duration  \\\n",
       "0             False  Execution returned non-zero exit code            0.0   \n",
       "1             False  Execution returned non-zero exit code            0.0   \n",
       "2             False  Execution returned non-zero exit code            0.0   \n",
       "3             False  Execution returned non-zero exit code            0.0   \n",
       "4             False  Execution returned non-zero exit code            0.0   \n",
       "\n",
       "   rollout_duration                                            rollout  \n",
       "0        186.208566  [{'turn_id': 0, 'action': '', 'observation': '...  \n",
       "1        188.979062  [{'turn_id': 0, 'action': '', 'observation': '...  \n",
       "2        194.721609  [{'turn_id': 0, 'action': '', 'observation': '...  \n",
       "3        197.420535  [{'turn_id': 0, 'action': '', 'observation': '...  \n",
       "4        198.732197  [{'turn_id': 0, 'action': '', 'observation': '...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten_dataframe(df_trj) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flatten the trajectory DataFrame into a simplified pandas DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with task_name, task_description, code, percentile, valid_submission,\n",
    "        eval_error_message, eval_duration, rollout_duration, and rollout columns.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for idx in range(len(df_trj)):\n",
    "        row = df_trj.iloc[idx]\n",
    "        rollout_data = row[\"rollouts\"][0]\n",
    "        transitions = rollout_data[\"traj\"][\"transitions\"]\n",
    "        last_outcomes = transitions[-1][\"outcomes\"]\n",
    "        info = transitions[-1][\"info\"]\n",
    "        \n",
    "        rows.append({\n",
    "            \"task_name\": rollout_data[\"start_args\"][\"instance_id\"],\n",
    "            \"task_description\": rollout_data[\"start_args\"][\"task_description\"],\n",
    "            \"code\": info.get(\"pred_solution\"),\n",
    "            \"percentile\": last_outcomes.get(\"percentile\"),\n",
    "            \"valid_submission\": last_outcomes.get(\"valid_submission\"),\n",
    "            \"eval_error_message\": last_outcomes.get(\"eval_error_message\"),\n",
    "            \"eval_duration\": last_outcomes.get(\"gpu_execution_duration\"),\n",
    "            \"rollout_duration\": rollout_data[\"metrics\"].get(\"rollout/duration\"),\n",
    "            \"rollout\": build_rollout(df_trj, idx)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Flatten the dataframe\n",
    "df_flat = flatten_dataframe(df_trj)\n",
    "\n",
    "print(f\"Shape: {df_flat.shape}\")\n",
    "print(f\"Columns: {df_flat.columns.tolist()}\")\n",
    "df_flat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ab197fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2526 rows to /checkpoint/maui_sft/winnieyangwn/amaia_dumps/513/trajectories/513_metadata.jsonl\n"
     ]
    }
   ],
   "source": [
    "save_path = f'/checkpoint/maui_sft/winnieyangwn/amaia_dumps/{run_id}/trajectories/{run_id}_metadata.jsonl'\n",
    "\n",
    "# Save df_flat to jsonl\n",
    "df_flat.to_json(save_path, orient='records', lines=True)\n",
    "print(f\"Saved {len(df_flat)} rows to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16204df1",
   "metadata": {},
   "source": [
    "# Data Structure Description\n",
    "\n",
    "The context is a list of dictionaries, where each dictionary represents one MLE Bench rollout (an agent's attempt to solve a machine learning task). There are 4,800 rollouts in total.\n",
    "\n",
    "Each rollout dictionary has the following fields:\n",
    "\n",
    "1. **\"task_name\"** (str): The unique identifier for the ML task (e.g., \"detecting-insults-in-social-commentary\")\n",
    "\n",
    "2. **\"task_description\"** (str): Full markdown description of the ML task including overview, evaluation criteria, and submission format\n",
    "\n",
    "3. **\"code\"** (str | None): The final Python code solution submitted by the agent\n",
    "\n",
    "4. **\"percentile\"** (float | None): Performance percentile (0-100) achieved by the submission. Higher is better. 100 means top performer.\n",
    "\n",
    "5. **\"valid_submission\"** (bool | None): Whether the agent produced a valid submission file\n",
    "\n",
    "6. **\"eval_error_message\"** (str | None): Evaluation result message - contains success info or error details\n",
    "\n",
    "7. **\"eval_duration\"** (float | None): GPU execution time in seconds for evaluation\n",
    "\n",
    "8. **\"rollout_duration\"** (float | None): Total time in seconds for the entire rollout\n",
    "\n",
    "9. **\"rollout\"** (list[dict]): The full conversation trajectory as a list of turns. Each turn has:\n",
    "   - \"turn_id\" (int): Turn number starting from 0\n",
    "   - \"action\" (str): The agent's action/response (typically bash commands in XML tags)\n",
    "   - \"observation\" (str): The environment's response/prompt to the agent\n",
    "\n",
    "## Example access patterns:\n",
    "```python\n",
    "# Get task name\n",
    "context[0][\"task_name\"]\n",
    "\n",
    "# Get percentile\n",
    "context[0][\"percentile\"]\n",
    "\n",
    "# Get number of turns\n",
    "len(context[0][\"rollout\"])\n",
    "\n",
    "# Get first action\n",
    "context[0][\"rollout\"][0][\"action\"]\n",
    "\n",
    "# Filter successful submissions\n",
    "[r for r in context if r[\"valid_submission\"] == True]\n",
    "\n",
    "# Filter by percentile\n",
    "[r for r in context if r[\"percentile\"] and r[\"percentile\"] >= 50]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7cb64c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8f6ba64",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amaia-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
