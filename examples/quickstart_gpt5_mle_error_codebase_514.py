"""
Quickstart example for analyzing MLE Bench rollout data with RLM.

This script demonstrates how to:
1. Load flattened MLE Bench trajectory data as a pandas DataFrame (FAST)
2. Provide a data schema description in the root_prompt
3. Query the RLM to analyze the rollout data

Performance optimization: Uses setup_code to load data directly into REPL,
bypassing JSON serialization of large context data.
"""

import os
import subprocess

from dotenv import load_dotenv

from rlm import RLM
from rlm.logger import RLMLogger

load_dotenv()

# =============================================================================
# Configuration
# =============================================================================
run_id = 514
DATA_PATH = f"/checkpoint/maui_sft/winnieyangwn/amaia_dumps/{run_id}/trajectories/{run_id}_metadata.jsonl"
CODEBASE_PATH = "/checkpoint/maui_sft/winnieyangwn/amaia_dumps/514/code/2026_01_23_09_50_18"  # Path to codebase directory
CODEBASE_EXTENSIONS = [".py", ".md", ".yaml"]  # File extensions to include (e.g., [".py", ".ts", ".js"])
CONFIG_YAML_PATH = "/home/winnieyangwn/amaia-collab/apps/sea/configs/winnieyang/eval/gpt5/514.yaml"  # Path to the YAML config file used to run the evaluation
model_name = "gpt-5"  # Example model name
job_name = "common_invalid_errors_codebase"
log_dir = "/checkpoint/maui_sft/winnieyangwn/rlm_dumps"


def get_row_count(path: str) -> int:
    """Get number of rows in JSONL file without loading it."""
    result = subprocess.run(["wc", "-l", path], capture_output=True, text=True)
    return int(result.stdout.split()[0])


def get_codebase_files(codebase_path: str, extensions: list[str]) -> list[str]:
    """Get list of files in codebase matching extensions."""
    from pathlib import Path
    files = []
    for ext in extensions:
        files.extend([str(p.relative_to(codebase_path)) for p in Path(codebase_path).rglob(f"*{ext}")])
    return sorted(files)


# =============================================================================
# Data Schema Description (for root_prompt)
# =============================================================================
def build_data_schema(num_rollouts: int, codebase_files: list[str], config_yaml_content: str) -> str:
    files_preview = "\n".join(f"  - {f}" for f in codebase_files[:20])
    if len(codebase_files) > 20:
        files_preview += f"\n  ... and {len(codebase_files) - 20} more files"
    
    return f"""
================================================================================
AVAILABLE VARIABLES (top-level, use directly - do NOT reassign these!)
================================================================================
The following variables are pre-loaded in the REPL namespace. Use them directly:
  - `rollout_df` (pandas.DataFrame) - MLE Bench rollout data
  - `codebase` (dict) - Source code files
  - `config_yaml` (str) - YAML configuration
  - `pd` (module) - pandas is already imported

⚠️ WARNING: Do NOT call globals() or locals() - they are disabled.
⚠️ WARNING: Do NOT reassign these variables (e.g., `rollout_df = ...`).
   Just use them directly: `rollout_df.head()`, `codebase.keys()`, etc.

================================================================================
1. ROLLOUT DATA: `rollout_df`
================================================================================
A pandas DataFrame with {num_rollouts} MLE Bench rollouts. Each rollout is an LLM agent's 
attempt to solve an ML task (from Kaggle competitions) through multi-turn interaction.
These rollouts were generated by executing the CODEBASE below with the CONFIG.

DATAFRAME COLUMNS:
├── task_name: str          # Task ID, e.g. "detecting-insults-in-social-commentary"
├── task_description: str   # Full task description (markdown)
├── code: str | None        # Final submitted Python solution
├── percentile: float | None  # Score 0-1 (higher = better, 1 = top)
├── valid_submission: bool  # Did agent produce valid submission?
├── eval_error_output: str  # Success/error details during evaluation
├── eval_duration: float    # GPU eval time (seconds)
├── rollout_duration: float # Total rollout time (seconds)
└── rollout: list[dict]     # Multi-turn interaction transcript (stored as Python list)
    ├── turn_id: int        # Turn number (0-indexed)
    ├── action: str         # Agent's response (reasoning + tool calls, e.g. bash commands)
    └── observation: str    # Environment's response to the action

ACCESS EXAMPLES:
  rollout_df["task_name"].iloc[0]                    # First rollout's task
  rollout_df["percentile"].iloc[0]                   # First rollout's score
  len(rollout_df["rollout"].iloc[0])                 # Number of turns in first rollout
  rollout_df["rollout"].iloc[0][0]["action"]         # First action of the first rollout
  rollout_df.groupby("task_name")["percentile"].mean()  # Avg score by task

================================================================================
2. CODEBASE: `codebase`
================================================================================
The source code that was used to run the MLE Bench evaluation and generate the rollouts above.
A dict mapping relative file paths to file contents.
   - {len(codebase_files)} files available
   - Files:
{files_preview}

ACCESS EXAMPLES:
  list(codebase.keys())                              # List all files
  codebase["path/to/file.py"]                        # Get file contents
  [f for f in codebase if "test" in f]               # Find test files

================================================================================
3. CONFIG: `config_yaml`
================================================================================
The YAML configuration file used to run the evaluation that generated the rollouts.
This defines parameters like model settings, environment configs, timeouts, etc.

CONFIG CONTENT:
```yaml
{config_yaml_content}
```

ACCESS: The config is also available as `config_yaml` (string) in the REPL.

================================================================================
IMPORTANT: HOW TO RETURN YOUR FINAL ANSWER
================================================================================
When returning your final answer, ALWAYS use FINAL_VAR instead of FINAL to avoid parsing issues:

1. Store your answer in a variable first:
   final_answer = "Your complete answer here..."

2. Then return it using FINAL_VAR:
   FINAL_VAR(final_answer)

DO NOT use FINAL(...) directly with content containing parentheses like "1)" or "2)" 
as this will truncate your answer.
"""


def main():
    # Get row count without loading data (fast)
    print(f"Counting rows in {DATA_PATH}...")
    num_rollouts = get_row_count(DATA_PATH)
    print(f"Found {num_rollouts} rollouts")

    # Get codebase file list
    print(f"Scanning codebase at {CODEBASE_PATH}...")
    codebase_files = get_codebase_files(CODEBASE_PATH, CODEBASE_EXTENSIONS)
    print(f"Found {len(codebase_files)} files")

    # Load config YAML
    print(f"Loading config from {CONFIG_YAML_PATH}...")
    with open(CONFIG_YAML_PATH, "r") as f:
        config_yaml_content = f.read()
    print(f"Loaded config ({len(config_yaml_content)} chars)")

    # Build schema description
    data_schema = build_data_schema(num_rollouts, codebase_files, config_yaml_content)

    # Set up logger
    logger = RLMLogger(log_dir=log_dir, file_name=f"{model_name}_{job_name}_{run_id}")

    # Setup code: load data directly into REPL (bypasses JSON serialization)
    extensions_str = str(CODEBASE_EXTENSIONS)
    # Escape the config content for embedding in setup code
    config_escaped = config_yaml_content.replace("\\", "\\\\").replace("'''", "\\'\\'\\'")
    setup_code = f"""
import pandas as pd
from pathlib import Path

# Load rollout data as DataFrame
rollout_df = pd.read_json('{DATA_PATH}', lines=True)

# Load codebase into dict
codebase = {{}}
codebase_root = Path('{CODEBASE_PATH}')
for ext in {extensions_str}:
    for path in codebase_root.rglob(f'*{{ext}}'):
        try:
            rel_path = str(path.relative_to(codebase_root))
            codebase[rel_path] = path.read_text(errors='ignore')
        except Exception:
            pass  # Skip unreadable files

# Load config YAML
config_yaml = '''{config_escaped}'''

print(f"Loaded {{len(rollout_df)}} rollouts, {{len(codebase)}} codebase files, and config YAML")
"""

    # Create the RLM Instance
    rlm = RLM(
        backend="azure_openai",
        backend_kwargs={
            "model_name": model_name,
            "api_key": os.getenv("AZURE_OPENAI_API_KEY"),
            "azure_endpoint": os.getenv("AZURE_OPENAI_ENDPOINT"),
            "azure_deployment": os.getenv("AZURE_OPENAI_DEPLOYMENT"),
            "api_version": "2025-03-01-preview",
        },
        environment="local",
        environment_kwargs={
            "setup_code": setup_code,  # Load data directly in REPL
        },
        max_depth=2,
        max_iterations=100,  # Reduced from 30 - simple analytical task
        logger=logger,
        verbose=True,
    )

    # Define your question
    # question = "What percentage of rollouts produced a valid submission?"
    # question = "Among all invalid submissions, what are the top 5 most common evaluation error messages?"
    question = "Examine the ROLLOUT DATA, Among all invalid submissions, what are the top 5 most common evaluation error messages? Then focusing on the top most frequent error. What is the likely cause to this error? Analyzing the CODEBASE, try to identify the root cause of the error. Suggest specific improvements to fix this error in future rollouts."

    # Build the root_prompt with data schema + question
    root_prompt = f"{data_schema}\nQUESTION: {question}"

    # Run RLM completion
    print(f"\nRunning RLM with question: {question}\n")
    result = rlm.completion(
        prompt="",                # Empty - data loaded via setup_code
        root_prompt=root_prompt   # Schema + question shown at each iteration
    )

    print("\n" + "=" * 80)
    print("FINAL RESULT:")
    print("=" * 80)
    print(result)


if __name__ == "__main__":
    main()
